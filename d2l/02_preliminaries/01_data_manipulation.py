# -*- coding:utf-8 -*-
import torch
"""
入门
"""
# 张量表示一个由数值组成的数组，这个数组可能有多个维度。
# 具有一个轴的张量对应数学上的向量（vector）；
# 具有两个轴的张量对应数学上的矩阵（matrix）；
# 具有两个轴以上的张量没有特殊的数学名称。
x = torch.arange(12)
print(x)

# 可以通过张量的shape属性来访问张量（沿每个轴的长度）的形状
print(x.shape)

# 获取张量中元素个数
print(x.numel())

# 改变张量的形状
# 我们可以通过-1来调用此自动计算出维度的功能。 即我们可以用x.reshape(-1,4)或x.reshape(3,-1)来取代x.reshape(3,4)。
X = x.reshape(3, 4)
print(X)
# tensor([[ 0,  1,  2,  3],
#         [ 4,  5,  6,  7],
#         [ 8,  9, 10, 11]])

# 常量张量
a = torch.zeros((2, 3, 4))  # 全0
b = torch.ones((2, 3, 4))  # 全1
c = torch.randn(3, 4)  # 均值为0，标准差为1的正态分布随机数

# 通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值。
# 最外层的列表对应于轴0，内层的列表对应于轴1。
d = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])

"""
运算符
"""
# 对于任意具有相同形状的张量， 常见的标准算术运算符（+、-、*、/和**）都可以被升级为按元素运算。
x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2])
print(x + y, x - y, x * y, x / y, x ** y) # **运算符是求幂运算
# tensor([ 3.,  4.,  6., 10.]) tensor([-1.,  0.,  2.,  6.]) tensor([ 2.,  4.,  8., 16.]) tensor([0.5000, 1.0000, 2.0000, 4.0000]) tensor([ 1.,  4., 16., 64.])
z = torch.exp(x)
print(z)
# tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])

# 多个张量连结（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。
# 我们只需要提供张量列表，并给出沿哪个轴连结。
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
print(torch.cat((X, Y), dim=0))
print(torch.cat((X, Y), dim=1))
# tensor([[ 0.,  1.,  2.,  3.],
#         [ 4.,  5.,  6.,  7.],
#         [ 8.,  9., 10., 11.],
#         [ 2.,  1.,  4.,  3.],
#         [ 1.,  2.,  3.,  4.],
#         [ 4.,  3.,  2.,  1.]])
# tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
#         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
#         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])

# 通过逻辑运算符构建二元张量
print(X == Y)
# tensor([[False,  True, False,  True],
#         [False, False, False, False],
#         [False, False, False, False]])

# 对张量中的所有元素进行求和，会产生一个单元素张量
print(X.sum())
# tensor(66.)

"""
广播机制
"""
# 在某些情况下，即使形状不同，我们仍然可以通过调用 广播机制（broadcasting mechanism）来执行按元素操作。
# 这种机制的工作方式如下：
# 1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；
# 2. 对生成的数组执行按元素操作。

# 在大多数情况下，我们将沿着数组中长度为1的轴进行广播
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
print(a, b)
# tensor([[0],
#         [1],
#         [2]])
# tensor([[0, 1]])
print(a + b) # 矩阵a将复制列， 矩阵b将复制行，然后再按元素相加。
# tensor([[0, 1],
#         [1, 2],
#         [2, 3]])


"""
索引和切片
"""
# 就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。
# 与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1； 可以指定范围以包含第一个元素和最后一个之前的元素。
X = torch.arange(12, dtype=torch.float32).reshape((3, 4))
print(X)
# tensor([[ 0.,  1.,  2.,  3.],
#         [ 4.,  5.,  6.,  7.],
#         [ 8.,  9., 10., 11.]])
print(X[-1], X[1:3])
# tensor([ 8.,  9., 10., 11.])
# tensor([[ 4.,  5.,  6.,  7.],
#         [ 8.,  9., 10., 11.]])

# 写入矩阵
X[1, 2] = 9
print(X)
# tensor([[ 0.,  1.,  2.,  3.],
#         [ 4.,  5.,  9.,  7.],
#         [ 8.,  9., 10., 11.]])

# 为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。
X[0:2, :] = 12 # [0:2, :]访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素
print(X)
# tensor([[12., 12., 12., 12.],
#         [12., 12., 12., 12.],
#         [ 8.,  9., 10., 11.]])

"""
节省内存，原地操作
"""
# 运行一些操作可能会导致为新结果分配内存。
# 例如，如果我们用Y = X + Y，我们将取消引用Y指向的张量，而是指向新分配的内存处的张量。
before = id(Y)
Y = Y + X
print(id(Y) == before) # False

# 这可能是不可取的，原因有两个：
# 1. 首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；
# 2. 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。

# 我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如Y[:] = <expression>。
Z = torch.zeros_like(Y)
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))

# 如果在后续计算中没有重复使用X， 我们也可以使用X[:] = X + Y或X += Y来减少操作的内存开销。
before = id(X)
X += Y
print(id(X) == before) # True

"""
转换为其他Python对象
"""
# 将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。
# torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。
A = X.numpy()
B = torch.tensor(A)
print(type(A), type(B))
# <class 'numpy.ndarray'> <class 'torch.Tensor'>
print(A, B)
# [[26. 25. 28. 27.]
#  [25. 26. 27. 28.]
#  [20. 21. 22. 23.]]
# tensor([[26., 25., 28., 27.],
#         [25., 26., 27., 28.],
#         [20., 21., 22., 23.]])
X[0, 0] = 100
print(A, B)
# [[100.  25.  28.  27.]
#  [ 25.  26.  27.  28.]
#  [ 20.  21.  22.  23.]]
# tensor([[26., 25., 28., 27.],
#         [25., 26., 27., 28.],
#         [20., 21., 22., 23.]])

# 要将大小为1的张量转换为Python标量，我们可以调用item函数或Python的内置函数。
a = torch.tensor([3.5])
print(a, a.item(), float(a), int(a))
# tensor([3.5000]) 3.5 3.5 3